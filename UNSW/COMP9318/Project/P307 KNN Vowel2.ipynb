{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P307 KNN Vowel2\n",
    "+ Try to improve F1 for Vowel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper\n",
    "import submission\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_to_list(curr_row):\n",
    "    word = curr_row[:curr_row.index(':')]\n",
    "    splitted_list = curr_row[curr_row.index(':') + 1 : ].split(' ')\n",
    "    return word, splitted_list\n",
    "\n",
    "def check(splitted_list):\n",
    "    count = 0\n",
    "    target = 0\n",
    "    removed_stress = []\n",
    "    for item in splitted_list[:]:\n",
    "        try: # vowel\n",
    "            curr = int(item[-1]) \n",
    "            removed_stress.append(item[:-1])\n",
    "            count += 1\n",
    "            if curr == 1:\n",
    "                target = count\n",
    "        except: # consonant\n",
    "            removed_stress.append(item)\n",
    "    return removed_stress, target\n",
    "def count_vowel(removed_stress, vowel):\n",
    "    count = 0\n",
    "    for item in removed_stress[:]:\n",
    "        if item in vowel:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singular_filter(word, removed_stress):\n",
    "    \n",
    "    ## 这里修改下, 若单词长度 <= 3, 不进行消除\n",
    "    if len(word) <= 3:\n",
    "        return word, removed_stress\n",
    "    \n",
    "    sub_string1 = word[-2: ]\n",
    "    sub_string2 = word[-3: ]\n",
    "    sub_string3 = word[-6: ]    \n",
    "    sub_string4 = word[-4: ]\n",
    "    \n",
    "    pattern1 = re.compile('[BCDFGHJKLMNOPQRTVWYZ]{1}S$')\n",
    "    pattern2 = re.compile('XS$')\n",
    "    # For CDFGHKLPWRTWYZ --> remove 'S' from word, remove 'S/Z' from removed_stress\n",
    "    if re.match(pattern1, sub_string1):\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    # For X --> remove 'S' from word, remove 'IH Z' from removed_stress\n",
    "    if re.match(pattern2, sub_string1):\n",
    "        return word[:-1], removed_stress[:-2]\n",
    "    \n",
    "    # -ES\n",
    "    pattern3 = re.compile('[ABDEFJKLMNOPQRTUWY]{1}ES$')\n",
    "    pattern4 = re.compile('IES$')\n",
    "    pattern5 = re.compile('HES$')\n",
    "    pattern55 = re.compile('ZZES$|SSES$')\n",
    "    pattern6 = re.compile('[CGSXZ]{1}ES$')\n",
    "    pattern7 = re.compile('SELVES$')\n",
    "    pattern8 = re.compile('IVES$')\n",
    "    pattern9 = re.compile('VES$')\n",
    "    \n",
    "    if re.match(pattern3, sub_string2): # ES --> E\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    if re.match(pattern4, sub_string2): # IES --> Y\n",
    "        word = word[:-3] + 'Y'\n",
    "        return word, removed_stress[:-1]\n",
    "    if re.match(pattern5, sub_string2):\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    if re.match(pattern55, sub_string4):\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    if re.match(pattern6, sub_string2):\n",
    "        return word[:-1], removed_stress[:-2]\n",
    "    if re.match(pattern7, sub_string3):\n",
    "        word = word[:-6] + 'SELF'\n",
    "        removed_stress.pop()\n",
    "        removed_stress.pop()\n",
    "        removed_stress.append('F')\n",
    "        return word, removed_stress\n",
    "    if re.match(pattern8, sub_string4) and (removed_stress[-3:] == ['AY', 'V', 'Z']):\n",
    "        word = word[:-3] + 'FE'\n",
    "        removed_stress.pop()\n",
    "        removed_stress.pop()\n",
    "        removed_stress.append('F')\n",
    "        return word, removed_stress\n",
    "    if re.match(pattern9, sub_string2):\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    \n",
    "    return word, removed_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neutral_filter1(word, removed_stress):\n",
    "    pattern1 = re.compile('ABL[EY]{1}$')\n",
    "    if re.search(pattern1, word):\n",
    "        return word[:-4], removed_stress[:-4]\n",
    "    \n",
    "    pattern2 = re.compile('LL[YI]{1}$|LLED$|SSED$|FFED$')\n",
    "    if re.search(pattern2, word):\n",
    "        return word[:-2], removed_stress[:-1]\n",
    "    \n",
    "    pattern3 = re.compile('L[YI]{1}$')\n",
    "    if re.search(pattern3, word) and removed_stress[-2:] == ['L', 'IY']:\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    \n",
    "    pattern4 = re.compile('DDED$|TTED$')\n",
    "    if re.search(pattern4, word):\n",
    "        return word[:-3], removed_stress[:-2]\n",
    "    \n",
    "    pattern5 = re.compile('[AEIOU]{1}[DFT]{1}ED$')\n",
    "    if re.search(pattern5, word):\n",
    "        return word[:-1], removed_stress[:-2]\n",
    "    \n",
    "    pattern6 = re.compile('[AEIOU]{1}[KMNS]{1}ED$|[IU]{1}RED$|IBED$')\n",
    "    if re.search(pattern6, word):\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    \n",
    "    pattern7 = re.compile('BBED$|GGED$|MMED$|NNED$|PPED$|RRED$')\n",
    "    if re.search(pattern7, word):\n",
    "        return word[:-3], removed_stress[:-1]\n",
    "    \n",
    "    pattern8 = re.compile('[SW]{1}EED$')\n",
    "    if re.search(pattern8, word):\n",
    "        return word[:-4], removed_stress[:-3]\n",
    "    \n",
    "    pattern9 = re.compile('B[LR]{1}EED$')\n",
    "    if re.search(pattern9, word):\n",
    "        return word[:-5], removed_stress[:-4]\n",
    "    \n",
    "    pattern10 = re.compile('[BHKMNOPSWXY]{1}ED$')\n",
    "    if re.search(pattern10, word):\n",
    "        return word[:-2], removed_stress[:-1]\n",
    "    \n",
    "    pattern11 = re.compile('IED$')\n",
    "    if re.search(pattern11, word):\n",
    "        word = word[:-3] + 'Y'\n",
    "        return word, removed_stress[:-1]\n",
    "    \n",
    "    pattern12 = re.compile('[CGLUVZ]{1}ED$')\n",
    "    if re.search(pattern12, word):\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    \n",
    "    pattern13 = re.compile('[DFTR]{1}ED$')\n",
    "    if re.search(pattern13, word):\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    \n",
    "    pattern14 = re.compile('ISM$')\n",
    "    if re.search(pattern14, word):\n",
    "        return word[:-3], removed_stress[:-4]    \n",
    "    \n",
    "    pattern15 = re.compile('FUL$|[AO]{1}LF$')\n",
    "    if re.search(pattern15, word):\n",
    "        return word[:-3], removed_stress[:-3]  \n",
    "    \n",
    "    pattern16 = re.compile('[NL]{1}ESS$') # remove 'NESS', remove 'N AH0 S'\n",
    "    if re.search(pattern16, word):\n",
    "        return word[:-4], removed_stress[:-3]\n",
    "    \n",
    "    pattern17 = re.compile('ING$')\n",
    "    if re.search(pattern17, word):\n",
    "        return word[:-3], removed_stress[:-2]\n",
    "    \n",
    "    return word, removed_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 只有singular 和neutral 1\n",
    "def combined_filter(word, removed_stress, vowel):\n",
    "    \n",
    "    if count_vowel(removed_stress, vowel) <= 2:\n",
    "        return word, removed_stress\n",
    "    word, removed_stress = singular_filter(word, removed_stress)\n",
    "    if len(word) == 0 or len(removed_stress) == 0: \n",
    "        return 1, [1]\n",
    "    \n",
    "    if count_vowel(removed_stress, vowel) <= 2:\n",
    "        return word, removed_stress\n",
    "    word, removed_stress = neutral_filter1(word, removed_stress)\n",
    "    if len(word) == 0 or len(removed_stress) == 0: \n",
    "        return 1, [1]\n",
    "    \n",
    "    count = 0\n",
    "    for item in removed_stress[:]:\n",
    "        if item in vowel:\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 1, [1]\n",
    "    \n",
    "    return word, removed_stress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_structure_and_vowels(removed_stress, vowel):\n",
    "    structure = ''\n",
    "    num_vowels = 0\n",
    "    count = 0\n",
    "    vowels = []\n",
    "    vowel_positions = []\n",
    "    for item in removed_stress[:]:\n",
    "        if item in vowel:\n",
    "            structure += 'V'\n",
    "            vowels.append(vowel.get(item))\n",
    "            vowel_positions.append(count)\n",
    "            num_vowels += 1\n",
    "            count += 1\n",
    "        else:\n",
    "            structure += 'C'\n",
    "            count += 1\n",
    "        \n",
    "    return vowels, vowel_positions, num_vowels, structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prefix_and_suffix(curr_vowel_position, removed_stress, consonant):\n",
    "    \n",
    "    if curr_vowel_position == 0: # no prefixes\n",
    "        prefix1 = 39\n",
    "    else:\n",
    "        prefix1 = consonant.get(removed_stress[curr_vowel_position - 1])\n",
    "    \n",
    "    try:\n",
    "        suffix1 = consonant.get(removed_stress[curr_vowel_position + 1])\n",
    "    except:\n",
    "        suffix1 = 39\n",
    "    return [prefix1, suffix1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##这里加入新函数\n",
    "def split_structure_vowel2(vowel_positions, removed_stress):\n",
    "    split_result = []\n",
    "    if vowel_positions[1] - vowel_positions[0] > 1: \n",
    "        split_result.append(' '.join(removed_stress[:vowel_positions[1] - 1]))\n",
    "        split_result.append(' '.join(removed_stress[vowel_positions[1] - 1 : ]))\n",
    "    else:\n",
    "        split_result.append(' '.join(removed_stress[:vowel_positions[0] + 1]))\n",
    "        split_result.append(' '.join(removed_stress[vowel_positions[0] + 1 : ]))\n",
    "    return split_result\n",
    "def split_structure_vowel3(vowel_positions, removed_stress):\n",
    "    if vowel_positions[1] - vowel_positions[0] > 1: # CVCCCVXXXX\n",
    "        part1 = removed_stress[:vowel_positions[1] - 1]\n",
    "        removed_stress = removed_stress[vowel_positions[1] - 1 : ]\n",
    "        \n",
    "    else: # CVVXXXX\n",
    "        part1 = removed_stress[:vowel_positions[0] + 1]\n",
    "        removed_stress = removed_stress[vowel_positions[0] + 1 : ]\n",
    "    \n",
    "    vowel_positions = [vowel_positions[1] - len(part1), vowel_positions[2] - len(part1)]\n",
    "    sub_split_result = split_structure_vowel2(vowel_positions, removed_stress)\n",
    "    split_result = [' '.join(part1)]\n",
    "    split_result.extend(sub_split_result)\n",
    "    return split_result\n",
    "def split_structure_vowel4(vowel_positions, removed_stress):\n",
    "    if vowel_positions[1] - vowel_positions[0] > 1: # CVCCCVXXXX\n",
    "        part1 = removed_stress[:vowel_positions[1] - 1]\n",
    "        removed_stress = removed_stress[vowel_positions[1] - 1 : ]\n",
    "        \n",
    "    else: # CVVXXXX\n",
    "        part1 = removed_stress[:vowel_positions[0] + 1]\n",
    "        removed_stress = removed_stress[vowel_positions[0] + 1 : ]\n",
    "    \n",
    "    vowel_positions = [vowel_positions[1] - len(part1), vowel_positions[2] - len(part1), vowel_positions[3] - len(part1)]\n",
    "    sub_split_result = split_structure_vowel3(vowel_positions, removed_stress)\n",
    "    split_result = [' '.join(part1)]\n",
    "    split_result.extend(sub_split_result)\n",
    "    return split_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据先按照元音数量进行切分, 然后按训练集测试集进行切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score,precision_score, recall_score\n",
    "vowel = {'AA': 0, 'AW': 1, 'AY': 2, 'ER': 3, 'EY': 4, 'IY': 5, 'OW': 6, 'OY': 7, 'UW': 8, 'AE': 9, 'AH': 10, 'AO': 11, 'EH': 12, 'IH': 13, 'UH': 14}\n",
    "consonant = {'AA': 0, 'AW': 1, 'AY': 2, 'ER': 3, 'EY': 4, 'IY': 5, 'OW': 6, 'OY': 7, 'UW': 8, 'AE': 9, 'AH': 10, 'AO': 11, 'EH': 12, 'IH': 13, 'UH': 14, 'P': 15, 'B': 16, 'CH': 17, 'D': 18, 'DH': 19, 'F': 20, 'G': 21, 'HH': 22, 'JH': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'NG': 28, 'R': 29, 'S': 30, 'SH': 31, 'T': 32, 'TH': 33, 'V': 34, 'W': 35, 'Y': 36, 'Z': 37, 'ZH': 38}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = helper.read_data('asset/training_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split raw data by vowel\n",
    "def split_raw_by_vowels(raw_data, vowel):\n",
    "    vowel2 = []\n",
    "    vowel3 = []\n",
    "    vowel4 = []\n",
    "    for curr_row in raw_data[:]:\n",
    "        word, splitted_list = split_to_list(curr_row)\n",
    "        removed_stress, target = check(splitted_list)\n",
    "        num_vowel = count_vowel(removed_stress, vowel)\n",
    "        if num_vowel == 2:\n",
    "            vowel2.append(word + ':' + ' '.join(splitted_list))\n",
    "        if num_vowel == 3:\n",
    "            vowel3.append(word + ':' + ' '.join(splitted_list))\n",
    "        if num_vowel == 4:\n",
    "            vowel4.append(word + ':' + ' '.join(splitted_list))\n",
    "    return vowel2, vowel3, vowel4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27619\n",
      "16395\n",
      "5986\n"
     ]
    }
   ],
   "source": [
    "vowel2, vowel3, vowel4 = split_raw_by_vowels(raw_data, vowel)\n",
    "print(len(vowel2))\n",
    "print(len(vowel3))\n",
    "print(len(vowel4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_structure_and_vowels2(removed_stress, vowel, consonant):\n",
    "    structure = ''\n",
    "    num_vowels = 0\n",
    "    count = 0\n",
    "    vowels = []\n",
    "    vowel_positions = []\n",
    "    VowelMap = []\n",
    "    ConsonantMap = []\n",
    "    VectorMap = [0] * 39\n",
    "    VectorMapCount = [0] * 39\n",
    "    for item in removed_stress[:]:\n",
    "        vowel_index = consonant.get(item)\n",
    "        VectorMap[vowel_index] = 1\n",
    "        VectorMapCount[vowel_index] += 1\n",
    "        if item in vowel:\n",
    "            structure += 'V'\n",
    "            vowels.append(vowel_index)\n",
    "            VowelMap.append(1)\n",
    "            ConsonantMap.append(0)\n",
    "            vowel_positions.append(count)\n",
    "            num_vowels += 1\n",
    "            count += 1\n",
    "        else:\n",
    "            VowelMap.append(0)\n",
    "            ConsonantMap.append(1)\n",
    "            structure += 'C'\n",
    "            count += 1\n",
    "            \n",
    "    if count < 15:\n",
    "        VowelMap += [0] * (15 - count)\n",
    "        ConsonantMap += [0] * (15 - count)\n",
    "            \n",
    "    return vowels, vowel_positions, structure, VowelMap, ConsonantMap, VectorMap, VectorMapCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 7, 10, 10]\n",
      "[1, 4, 6, 8]\n",
      "CVCCVCVCVC\n",
      "[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "rss = ['N', 'AA', 'N', 'P', 'OY', 'Z', 'AH', 'N', 'AH', 'S']\n",
    "vowels, vowel_positions, structure, VowelMap, ConsonantMap, VectorMap, VectorMapCount = get_structure_and_vowels2(rss, vowel, consonant)\n",
    "for item in [vowels, vowel_positions, structure, VowelMap, ConsonantMap, VectorMap, VectorMapCount]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N AA N', 'P OY', 'Z AH', 'N AH S']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "split_structure_vowel4(vowel_positions, rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_vowel2(vowel2, vowel, consonant):\n",
    "    data2 = []\n",
    "    structure_dict = {}\n",
    "    syllable_dict = {}\n",
    "    \n",
    "    for curr_row in vowel2[:]:\n",
    "        word, splitted_list = split_to_list(curr_row)\n",
    "        removed_stress, target = check(splitted_list)\n",
    "        word, removed_stress = combined_filter(word, removed_stress, vowel)\n",
    "        # word, removed_stress = singular_filter(word, removed_stress)\n",
    "        \n",
    "        if type(word) == int:\n",
    "            continue\n",
    "        else:\n",
    "            vowels, vowel_positions, structure, VowelMap, ConsonantMap, VectorMap, VectorMapCount = get_structure_and_vowels2(removed_stress, vowel, consonant)\n",
    "            \n",
    "            # some word may be removed after filtering\n",
    "            if len(vowels) != 2:\n",
    "                continue\n",
    "            \n",
    "            # get the prefix and suffix\n",
    "            prefix_suffix = []\n",
    "            for curr_vowel_position in vowel_positions[:]:\n",
    "                p_s = get_prefix_and_suffix(curr_vowel_position, removed_stress, consonant)\n",
    "                prefix_suffix.extend(p_s)\n",
    "            \n",
    "            # get the structure of vowel\n",
    "            \n",
    "            if structure in structure_dict:\n",
    "                structure_index = structure_dict.get(structure)\n",
    "            else:\n",
    "                structure_index = len(structure_dict)\n",
    "                structure_dict[structure] = structure_index\n",
    "            \n",
    "            # get the syllables\n",
    "            split_result = split_structure_vowel2(vowel_positions, removed_stress)\n",
    "            syllable_indexes = []\n",
    "            for syllable in split_result[:]:\n",
    "                if syllable in syllable_dict:\n",
    "                    syllable_index = syllable_dict.get(syllable)\n",
    "                else:\n",
    "                    syllable_index = len(syllable_dict)\n",
    "                    syllable_dict[syllable] = syllable_index\n",
    "                syllable_indexes.append(syllable_index)\n",
    "            \n",
    "            # build the row data\n",
    "            row_data = [target, word, ' '.join(removed_stress), structure, structure_index, vowels, prefix_suffix, VowelMap, ConsonantMap, VectorMap, split_result, syllable_indexes]\n",
    "            \n",
    "            data2.append(row_data)\n",
    "    \n",
    "    return data2, structure_dict, syllable_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2, structure_dict, syllable_dict = get_features_vowel2(vowel2, vowel, consonant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27619"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_df2(data2):\n",
    "    features = ['Target','Word','RemovedStress', 'Structure', 'StructureIndex', 'Vowels', 'PrefixSuffix', 'VowelPositionMap', 'ConsonantPositionMap', 'VectorMap', 'Syllables','SyllableIndex']\n",
    "    df = pd.DataFrame(data2, columns = features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = get_df2(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Word</th>\n",
       "      <th>RemovedStress</th>\n",
       "      <th>Structure</th>\n",
       "      <th>StructureIndex</th>\n",
       "      <th>Vowels</th>\n",
       "      <th>PrefixSuffix</th>\n",
       "      <th>VowelPositionMap</th>\n",
       "      <th>ConsonantPositionMap</th>\n",
       "      <th>VectorMap</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>SyllableIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>COED</td>\n",
       "      <td>K OW EH D</td>\n",
       "      <td>CVVC</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 12]</td>\n",
       "      <td>[24, 12, 6, 18]</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[K OW, EH D]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PURVIEW</td>\n",
       "      <td>P ER V Y UW</td>\n",
       "      <td>CVCCV</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 8]</td>\n",
       "      <td>[15, 34, 36, 39]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[P ER V, Y UW]</td>\n",
       "      <td>[2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>HEHIR</td>\n",
       "      <td>HH EH HH IH R</td>\n",
       "      <td>CVCVC</td>\n",
       "      <td>2</td>\n",
       "      <td>[12, 13]</td>\n",
       "      <td>[22, 22, 22, 29]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[HH EH, HH IH R]</td>\n",
       "      <td>[4, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target     Word  RemovedStress Structure  StructureIndex    Vowels  \\\n",
       "0       1     COED      K OW EH D      CVVC               0   [6, 12]   \n",
       "1       1  PURVIEW    P ER V Y UW     CVCCV               1    [3, 8]   \n",
       "2       1    HEHIR  HH EH HH IH R     CVCVC               2  [12, 13]   \n",
       "\n",
       "       PrefixSuffix                               VowelPositionMap  \\\n",
       "0   [24, 12, 6, 18]  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [15, 34, 36, 39]  [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [22, 22, 22, 29]  [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                            ConsonantPositionMap  \\\n",
       "0  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                           VectorMap         Syllables  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...      [K OW, EH D]   \n",
       "1  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...    [P ER V, Y UW]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  [HH EH, HH IH R]   \n",
       "\n",
       "  SyllableIndex  \n",
       "0        [0, 1]  \n",
       "1        [2, 3]  \n",
       "2        [4, 5]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到训练模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combination(Syllables):\n",
    "    combination = []\n",
    "    length = len(Syllables)\n",
    "    for curr_length in range(1,length + 1):\n",
    "        for start_position in range(length - curr_length + 1):\n",
    "            combination.insert(0, ' '.join(Syllables[start_position : start_position + curr_length]))\n",
    "    return combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N AY JH IH R IY AH N',\n",
       " 'JH IH R IY AH N',\n",
       " 'N AY JH IH R IY',\n",
       " 'R IY AH N',\n",
       " 'JH IH R IY',\n",
       " 'N AY JH IH',\n",
       " 'AH N',\n",
       " 'R IY',\n",
       " 'JH IH',\n",
       " 'N AY']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "Syls = ['N AY', 'JH IH', 'R IY', 'AH N']\n",
    "get_combination(Syls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vowels_to_bin(Vowels, vowel):\n",
    "    result = []\n",
    "    for item in Vowels[:]:\n",
    "        curr = [0] * 15\n",
    "        curr[item] += 1\n",
    "        result.extend(curr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefix_suffix_to_bin(PrefixSuffix, consonant):\n",
    "    result = []\n",
    "    for item in PrefixSuffix[:]:\n",
    "        curr = [0] * 40 # note \"39\" is for boundary\n",
    "        curr[item] += 1\n",
    "        result.extend(curr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['VowelMap'] = df2.Vowels.apply(vowels_to_bin, vowel = vowel)\n",
    "df2['PrefixSuffixMap'] = df2.PrefixSuffix.apply(prefix_suffix_to_bin, consonant = consonant)\n",
    "df2['SyllableCombination'] = df2.Syllables.apply(get_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Word</th>\n",
       "      <th>RemovedStress</th>\n",
       "      <th>Structure</th>\n",
       "      <th>StructureIndex</th>\n",
       "      <th>Vowels</th>\n",
       "      <th>PrefixSuffix</th>\n",
       "      <th>VowelPositionMap</th>\n",
       "      <th>ConsonantPositionMap</th>\n",
       "      <th>VectorMap</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>SyllableIndex</th>\n",
       "      <th>VowelMap</th>\n",
       "      <th>PrefixSuffixMap</th>\n",
       "      <th>SyllableCombination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>COED</td>\n",
       "      <td>K OW EH D</td>\n",
       "      <td>CVVC</td>\n",
       "      <td>0</td>\n",
       "      <td>[6, 12]</td>\n",
       "      <td>[24, 12, 6, 18]</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[K OW, EH D]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[K OW EH D, EH D, K OW]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PURVIEW</td>\n",
       "      <td>P ER V Y UW</td>\n",
       "      <td>CVCCV</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 8]</td>\n",
       "      <td>[15, 34, 36, 39]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[P ER V, Y UW]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[P ER V Y UW, Y UW, P ER V]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>HEHIR</td>\n",
       "      <td>HH EH HH IH R</td>\n",
       "      <td>CVCVC</td>\n",
       "      <td>2</td>\n",
       "      <td>[12, 13]</td>\n",
       "      <td>[22, 22, 22, 29]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[HH EH, HH IH R]</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[HH EH HH IH R, HH IH R, HH EH]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target     Word  RemovedStress Structure  StructureIndex    Vowels  \\\n",
       "0       1     COED      K OW EH D      CVVC               0   [6, 12]   \n",
       "1       1  PURVIEW    P ER V Y UW     CVCCV               1    [3, 8]   \n",
       "2       1    HEHIR  HH EH HH IH R     CVCVC               2  [12, 13]   \n",
       "\n",
       "       PrefixSuffix                               VowelPositionMap  \\\n",
       "0   [24, 12, 6, 18]  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [15, 34, 36, 39]  [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [22, 22, 22, 29]  [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                            ConsonantPositionMap  \\\n",
       "0  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                           VectorMap         Syllables  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...      [K OW, EH D]   \n",
       "1  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...    [P ER V, Y UW]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  [HH EH, HH IH R]   \n",
       "\n",
       "  SyllableIndex                                           VowelMap  \\\n",
       "0        [0, 1]  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1        [2, 3]  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2        [4, 5]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                                     PrefixSuffixMap  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "               SyllableCombination  \n",
       "0          [K OW EH D, EH D, K OW]  \n",
       "1      [P ER V Y UW, Y UW, P ER V]  \n",
       "2  [HH EH HH IH R, HH IH R, HH EH]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2.PrefixSuffixMap[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpacked Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unpacked_dataset0 = pd.DataFrame.from_records(df2.SyllableCombination.tolist(),columns = ['C12','C2','C1'])\n",
    "unpacked_dataset1 = pd.DataFrame.from_records(df2.VowelPositionMap.tolist(),columns = range(1,16))\n",
    "unpacked_dataset2 = pd.DataFrame.from_records(df2.ConsonantPositionMap.tolist(),columns = range(16,31))\n",
    "unpacked_dataset3 = pd.DataFrame.from_records(df2.VectorMap.tolist(),columns = range(31, 70))\n",
    "unpacked_dataset4 = pd.DataFrame.from_records(df2.VowelMap.tolist(),columns = range(100,130))\n",
    "unpacked_dataset5 = pd.DataFrame.from_records(df2.PrefixSuffixMap.tolist(),columns = range(1000,1160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_dataset = pd.concat([df2.Target, unpacked_dataset0], axis=1) # Syllable Combinations\n",
    "final_dataset = pd.concat([final_dataset, unpacked_dataset1], axis=1) # VowelPositionMap\n",
    "final_dataset = pd.concat([final_dataset, unpacked_dataset2], axis=1) # ConsonantPositionMap\n",
    "final_dataset = pd.concat([final_dataset, unpacked_dataset3], axis=1) # VectorMap\n",
    "final_dataset = pd.concat([final_dataset, unpacked_dataset4], axis=1) # VowelMap\n",
    "final_dataset = pd.concat([final_dataset, unpacked_dataset5], axis=1) # PrefixSuffixMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>C12</th>\n",
       "      <th>C2</th>\n",
       "      <th>C1</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "      <th>1156</th>\n",
       "      <th>1157</th>\n",
       "      <th>1158</th>\n",
       "      <th>1159</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>K OW EH D</td>\n",
       "      <td>EH D</td>\n",
       "      <td>K OW</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P ER V Y UW</td>\n",
       "      <td>Y UW</td>\n",
       "      <td>P ER V</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>HH EH HH IH R</td>\n",
       "      <td>HH IH R</td>\n",
       "      <td>HH EH</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target            C12       C2      C1  1  2  3  4  5  6  ...   1150  1151  \\\n",
       "0       1      K OW EH D     EH D    K OW  0  1  1  0  0  0  ...      0     0   \n",
       "1       1    P ER V Y UW     Y UW  P ER V  0  1  0  0  1  0  ...      0     0   \n",
       "2       1  HH EH HH IH R  HH IH R   HH EH  0  1  0  1  0  0  ...      0     0   \n",
       "\n",
       "   1152  1153  1154  1155  1156  1157  1158  1159  \n",
       "0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     1  \n",
       "2     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[3 rows x 263 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(final_dataset, test_size = 0.1)\n",
    "features = list(final_dataset.columns)[4:]\n",
    "train_X = train[features]\n",
    "train_y = train['Target']\n",
    "test_X = test[features]\n",
    "test_y = test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5\n",
      "Training acc : 0.998390795349\n",
      "Testing acc : 0.929036929761\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-c55a6070d86d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training acc : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing acc : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    380\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m             )\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5, 100, 5):\n",
    "    print ('k = ' + str(i))\n",
    "    neigh = KNeighborsClassifier(n_neighbors=i, weights='distance')\n",
    "    neigh.fit(train_X, train_y)\n",
    "    print('Training acc : ' + str(neigh.score(train_X, train_y)))\n",
    "    print('Testing acc : ' + str(neigh.score(test_X, test_y)))\n",
    "    pred = neigh.predict(test_X)\n",
    "    print('F1 : ' + str(f1_score(list(test_y), pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round : 1\n",
      "Training acc : 0.998390795349\n",
      "Testing acc : 0.933381607531\n",
      "F1 : 0.802648399699\n",
      "= = = = = Round End = = = = =\n",
      "Round : 2\n",
      "Training acc : 0.998390795349\n",
      "Testing acc : 0.935916002896\n",
      "F1 : 0.796155987116\n",
      "= = = = = Round End = = = = =\n",
      "Round : 3\n",
      "Training acc : 0.998471255582\n",
      "Testing acc : 0.92867487328\n",
      "F1 : 0.799482966072\n",
      "= = = = = Round End = = = = =\n",
      "Round : 4\n",
      "Training acc : 0.998431025466\n",
      "Testing acc : 0.935191889935\n",
      "F1 : 0.81097547221\n",
      "= = = = = Round End = = = = =\n",
      "Round : 5\n",
      "Training acc : 0.998270105001\n",
      "Testing acc : 0.929036929761\n",
      "F1 : 0.804638567348\n",
      "= = = = = Round End = = = = =\n",
      "Average F1 : 0.802780278489\n"
     ]
    }
   ],
   "source": [
    "## k = 5\n",
    "f1 = 0\n",
    "for round in range(1,6):\n",
    "    print(\"Round : \" + str(round))\n",
    "    train, test = train_test_split(final_dataset, test_size = 0.1)\n",
    "    features = list(final_dataset.columns)[4:]\n",
    "    train_X = train[features]\n",
    "    train_y = train['Target']\n",
    "    test_X = test[features]\n",
    "    test_y = test['Target']\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "    neigh.fit(train_X, train_y)\n",
    "    print('Training acc : ' + str(neigh.score(train_X, train_y)))\n",
    "    print('Testing acc : ' + str(neigh.score(test_X, test_y)))\n",
    "    pred = neigh.predict(test_X)\n",
    "    curr_f1 = f1_score(list(test_y), pred, average='macro')\n",
    "    f1 += curr_f1\n",
    "    print('F1 : ' + str(curr_f1))\n",
    "    print(\"= = = = = Round End = = = = =\")\n",
    "print(\"Average F1 : \" + str(f1 * 1.0 / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We can find that k = 7 is good, now we try to get more detailed dataset\n",
    "+ 每次通过Syllables看能不能找到更精确的Syllables\n",
    "## 对每个训练样本, 执行搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_refined_df(test_combination, df):\n",
    "    df_features = ['C12','C2','C1']\n",
    "    for i in range(len(df_features)):\n",
    "        sub_df = df[df[df_features[i]].str.contains('^' + test_combination[i] + '$')]\n",
    "        if sub_df.shape[0] > 0:\n",
    "            # print(\"Pattern : \" + df_features[i] + \" , \" + test_combination[i])\n",
    "            return sub_df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern : C1234 , N AY JH IH R IY AH N\n"
     ]
    }
   ],
   "source": [
    "# test_combination\n",
    "test_comb = ['N AY JH IH R IY AH N', 'JH IH R IY AH N', 'N AY JH IH R IY', 'R IY AH N', 'JH IH R IY', 'N AY JH IH', 'AH N', 'R IY', 'JH IH', 'N AY']\n",
    "sub_df = get_refined_df(test_comb, final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-402c38257ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msub_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sub_df' is not defined"
     ]
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now transform the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(final_dataset, test_size = 0.1)\n",
    "features = list(final_dataset.columns)[11:]\n",
    "syllables_comb = list(final_dataset.columns)[1:11]\n",
    "\n",
    "test_y = list(test.Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(train, test, features):\n",
    "    pred = []\n",
    "    test_X_total = test[features]\n",
    "    test_syllables_comb = test[syllables_comb]\n",
    "    for i in range(test.shape[0]):\n",
    "        curr_X = np.array(test_X_total.iloc[i]).reshape((1,-1))\n",
    "        curr_syllables_comb = list(test_syllables_comb.iloc[i])\n",
    "        \n",
    "        sub_train = get_refined_df(curr_syllables_comb, train)\n",
    "        train_X = sub_train[features]\n",
    "        train_y = sub_train[\"Target\"]\n",
    "        \n",
    "        if train_X.shape[0] < 5:\n",
    "            neigh = KNeighborsClassifier(n_neighbors = train_X.shape[0], weights = 'distance')\n",
    "            # neigh = KNeighborsClassifier(n_neighbors = 1, weights = 'distance')\n",
    "            \n",
    "        else:\n",
    "            neigh = KNeighborsClassifier(n_neighbors = 5, weights = 'distance')\n",
    "        neigh.fit(train_X, train_y)\n",
    "        curr_pred = neigh.predict(curr_X)[0]\n",
    "        pred.append(curr_pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round : 0\n",
      "F1 : 0.805206220768\n",
      "Round : 1\n",
      "F1 : 0.795801249861\n",
      "Round : 2\n",
      "F1 : 0.800511559909\n",
      "Round : 3\n",
      "F1 : 0.818495411583\n",
      "Round : 4\n",
      "F1 : 0.794038820046\n",
      "Average F1 : 0.802810652433\n"
     ]
    }
   ],
   "source": [
    "f1 = 0\n",
    "for i in range(5):\n",
    "    print(\"Round : \" + str(i))\n",
    "    train, test = train_test_split(final_dataset, test_size = 0.1)\n",
    "    features = list(final_dataset.columns)[4:]\n",
    "    syllables_comb = list(final_dataset.columns)[1:4]\n",
    "    test_y = list(test.Target)\n",
    "    pred = test_model(train, test, features)\n",
    "    curr_f1 = f1_score(test_y, pred, average='macro')\n",
    "    f1 += curr_f1\n",
    "    print('F1 : ' + str(curr_f1))\n",
    "print('Average F1 : ' + str(f1 * 1.0 / 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
