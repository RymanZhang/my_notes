{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P308 LR Vowel2\n",
    "+ Try to improve F1 for Vowel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper\n",
    "import submission\n",
    "import re\n",
    "import pickle\n",
    "def split_to_list(curr_row):\n",
    "    word = curr_row[:curr_row.index(':')]\n",
    "    splitted_list = curr_row[curr_row.index(':') + 1 : ].split(' ')\n",
    "    return word, splitted_list\n",
    "\n",
    "def check(splitted_list):\n",
    "    count = 0\n",
    "    target = 0\n",
    "    removed_stress = []\n",
    "    for item in splitted_list[:]:\n",
    "        try: # vowel\n",
    "            curr = int(item[-1]) \n",
    "            removed_stress.append(item[:-1])\n",
    "            count += 1\n",
    "            if curr == 1:\n",
    "                target = count\n",
    "        except: # consonant\n",
    "            removed_stress.append(item)\n",
    "    return removed_stress, target\n",
    "def count_vowel(removed_stress, vowel):\n",
    "    count = 0\n",
    "    for item in removed_stress[:]:\n",
    "        if item in vowel:\n",
    "            count += 1\n",
    "    return count\n",
    "def singular_filter_sbbb(word, removed_stress, splitted_list):\n",
    "    \n",
    "    ## 这里修改下, 若单词长度 <= 3, 不进行消除\n",
    "    if len(word) <= 3:\n",
    "        return word, removed_stress\n",
    "    \n",
    "    sub_string1 = word[-2: ]\n",
    "    sub_string2 = word[-3: ]\n",
    "    sub_string3 = word[-6: ]    \n",
    "    sub_string4 = word[-4: ]\n",
    "    \n",
    "    pattern1 = re.compile('[BCDFGHJKLMNOPQRTVWYZ]{1}S$')\n",
    "    pattern2 = re.compile('XS$')\n",
    "    # For CDFGHKLPWRTWYZ --> remove 'S' from word, remove 'S/Z' from removed_stress\n",
    "    if re.match(pattern1, sub_string1):\n",
    "        return word[:-1], splitted_list[:-1]\n",
    "    # For X --> remove 'S' from word, remove 'IH Z' from removed_stress\n",
    "    if re.match(pattern2, sub_string1):\n",
    "        return word[:-1], splitted_list[:-2]\n",
    "    \n",
    "    # -ES\n",
    "    pattern3 = re.compile('[ABDEFJKLMNOPQRTUWY]{1}ES$')\n",
    "    pattern4 = re.compile('IES$')\n",
    "    pattern5 = re.compile('HES$')\n",
    "    pattern55 = re.compile('ZZES$|SSES$')\n",
    "    pattern6 = re.compile('[CGSXZ]{1}ES$')\n",
    "    pattern7 = re.compile('SELVES$')\n",
    "    pattern8 = re.compile('IVES$')\n",
    "    pattern9 = re.compile('VES$')\n",
    "    \n",
    "    if re.match(pattern3, sub_string2): # ES --> E\n",
    "        return word[:-1], splitted_list[:-1]\n",
    "    if re.match(pattern4, sub_string2): # IES --> Y\n",
    "        word = word[:-3] + 'Y'\n",
    "        return word, removed_stress[:-1]\n",
    "    if re.match(pattern5, sub_string2):\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    if re.match(pattern55, sub_string4):\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    if re.match(pattern6, sub_string2):\n",
    "        return word[:-1], removed_stress[:-2]\n",
    "    if re.match(pattern7, sub_string3):\n",
    "        word = word[:-6] + 'SELF'\n",
    "        removed_stress.pop()\n",
    "        removed_stress.pop()\n",
    "        removed_stress.append('F')\n",
    "        return word, removed_stress\n",
    "    if re.match(pattern8, sub_string4) and (removed_stress[-3:] == ['AY', 'V', 'Z']):\n",
    "        word = word[:-3] + 'FE'\n",
    "        removed_stress.pop()\n",
    "        removed_stress.pop()\n",
    "        removed_stress.append('F')\n",
    "        return word, removed_stress\n",
    "    if re.match(pattern9, sub_string2):\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    \n",
    "    return word, removed_stress\n",
    "def neutral_filter1(word, removed_stress):\n",
    "    pattern1 = re.compile('ABL[EY]{1}$')\n",
    "    if re.search(pattern1, word):\n",
    "        return word[:-4], removed_stress[:-4]\n",
    "    \n",
    "    pattern2 = re.compile('LL[YI]{1}$|LLED$|SSED$|FFED$')\n",
    "    if re.search(pattern2, word):\n",
    "        return word[:-2], removed_stress[:-1]\n",
    "    \n",
    "    pattern3 = re.compile('L[YI]{1}$')\n",
    "    if re.search(pattern3, word) and removed_stress[-2:] == ['L', 'IY']:\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    \n",
    "    pattern4 = re.compile('DDED$|TTED$')\n",
    "    if re.search(pattern4, word):\n",
    "        return word[:-3], removed_stress[:-2]\n",
    "    \n",
    "    pattern5 = re.compile('[AEIOU]{1}[DFT]{1}ED$')\n",
    "    if re.search(pattern5, word):\n",
    "        return word[:-1], removed_stress[:-2]\n",
    "    \n",
    "    pattern6 = re.compile('[AEIOU]{1}[KMNS]{1}ED$|[IU]{1}RED$|IBED$')\n",
    "    if re.search(pattern6, word):\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    \n",
    "    pattern7 = re.compile('BBED$|GGED$|MMED$|NNED$|PPED$|RRED$')\n",
    "    if re.search(pattern7, word):\n",
    "        return word[:-3], removed_stress[:-1]\n",
    "    \n",
    "    pattern8 = re.compile('[SW]{1}EED$')\n",
    "    if re.search(pattern8, word):\n",
    "        return word[:-4], removed_stress[:-3]\n",
    "    \n",
    "    pattern9 = re.compile('B[LR]{1}EED$')\n",
    "    if re.search(pattern9, word):\n",
    "        return word[:-5], removed_stress[:-4]\n",
    "    \n",
    "    pattern10 = re.compile('[BHKMNOPSWXY]{1}ED$')\n",
    "    if re.search(pattern10, word):\n",
    "        return word[:-2], removed_stress[:-1]\n",
    "    \n",
    "    pattern11 = re.compile('IED$')\n",
    "    if re.search(pattern11, word):\n",
    "        word = word[:-3] + 'Y'\n",
    "        return word, removed_stress[:-1]\n",
    "    \n",
    "    pattern12 = re.compile('[CGLUVZ]{1}ED$')\n",
    "    if re.search(pattern12, word):\n",
    "        return word[:-1], removed_stress[:-1]\n",
    "    \n",
    "    pattern13 = re.compile('[DFTR]{1}ED$')\n",
    "    if re.search(pattern13, word):\n",
    "        return word[:-2], removed_stress[:-2]\n",
    "    \n",
    "    pattern14 = re.compile('ISM$')\n",
    "    if re.search(pattern14, word):\n",
    "        return word[:-3], removed_stress[:-4]    \n",
    "    \n",
    "    pattern15 = re.compile('FUL$|[AO]{1}LF$')\n",
    "    if re.search(pattern15, word):\n",
    "        return word[:-3], removed_stress[:-3]  \n",
    "    \n",
    "    pattern16 = re.compile('[NL]{1}ESS$') # remove 'NESS', remove 'N AH0 S'\n",
    "    if re.search(pattern16, word):\n",
    "        return word[:-4], removed_stress[:-3]\n",
    "    \n",
    "    pattern17 = re.compile('ING$')\n",
    "    if re.search(pattern17, word):\n",
    "        return word[:-3], removed_stress[:-2]\n",
    "    \n",
    "    return word, removed_stress\n",
    "# 只有singular 和neutral 1\n",
    "def combined_filter(word, removed_stress, vowel):\n",
    "    \n",
    "    if count_vowel(removed_stress, vowel) <= 2:\n",
    "        return word, removed_stress\n",
    "    word, removed_stress = singular_filter(word, removed_stress)\n",
    "    if len(word) == 0 or len(removed_stress) == 0: \n",
    "        return 1, [1]\n",
    "    \n",
    "    if count_vowel(removed_stress, vowel) <= 2:\n",
    "        return word, removed_stress\n",
    "    word, removed_stress = neutral_filter1(word, removed_stress)\n",
    "    if len(word) == 0 or len(removed_stress) == 0: \n",
    "        return 1, [1]\n",
    "    \n",
    "    count = 0\n",
    "    for item in removed_stress[:]:\n",
    "        if item in vowel:\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 1, [1]\n",
    "    \n",
    "    return word, removed_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27619\n",
      "16395\n",
      "5986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score,precision_score, recall_score\n",
    "vowel = {'AA': 0, 'AW': 1, 'AY': 2, 'ER': 3, 'EY': 4, 'IY': 5, 'OW': 6, 'OY': 7, 'UW': 8, 'AE': 9, 'AH': 10, 'AO': 11, 'EH': 12, 'IH': 13, 'UH': 14}\n",
    "consonant = {'AA': 0, 'AW': 1, 'AY': 2, 'ER': 3, 'EY': 4, 'IY': 5, 'OW': 6, 'OY': 7, 'UW': 8, 'AE': 9, 'AH': 10, 'AO': 11, 'EH': 12, 'IH': 13, 'UH': 14, 'P': 15, 'B': 16, 'CH': 17, 'D': 18, 'DH': 19, 'F': 20, 'G': 21, 'HH': 22, 'JH': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'NG': 28, 'R': 29, 'S': 30, 'SH': 31, 'T': 32, 'TH': 33, 'V': 34, 'W': 35, 'Y': 36, 'Z': 37, 'ZH': 38}\n",
    "raw_data = helper.read_data('asset/training_data.txt')\n",
    "# split raw data by vowel\n",
    "def split_raw_by_vowels(raw_data, vowel):\n",
    "    vowel2 = []\n",
    "    vowel3 = []\n",
    "    vowel4 = []\n",
    "    for curr_row in raw_data[:]:\n",
    "        word, splitted_list = split_to_list(curr_row)\n",
    "        removed_stress, target = check(splitted_list)\n",
    "        num_vowel = count_vowel(removed_stress, vowel)\n",
    "        if num_vowel == 2:\n",
    "            vowel2.append(word + ':' + ' '.join(splitted_list))\n",
    "        if num_vowel == 3:\n",
    "            vowel3.append(word + ':' + ' '.join(splitted_list))\n",
    "        if num_vowel == 4:\n",
    "            vowel4.append(word + ':' + ' '.join(splitted_list))\n",
    "    return vowel2, vowel3, vowel4\n",
    "vowel2, vowel3, vowel4 = split_raw_by_vowels(raw_data, vowel)\n",
    "print(len(vowel2))\n",
    "print(len(vowel3))\n",
    "print(len(vowel4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COED:K OW1 EH2 D', 'PURVIEW:P ER1 V Y UW2', 'HEHIR:HH EH1 HH IH0 R']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowel2[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR on Vowel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import time\n",
    "import submissionLRvowel2 as submission\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "classifier_path = './asset/classifier.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_to_list(curr_row):\n",
    "    word = curr_row[:curr_row.index(':')]\n",
    "    splitted_list = curr_row[curr_row.index(':') + 1 : ].split(' ')\n",
    "    return word, splitted_list\n",
    "def check(splitted_list):\n",
    "    count = 0\n",
    "    target = 0\n",
    "    removed_stress = []\n",
    "    for item in splitted_list[:]:\n",
    "        try: # vowel\n",
    "            curr = int(item[-1]) \n",
    "            removed_stress.append(item[:-1])\n",
    "            count += 1\n",
    "            if curr == 1:\n",
    "                target = count\n",
    "        except: # consonant\n",
    "            removed_stress.append(item)\n",
    "    return removed_stress, target\n",
    "def get_test_data(test_data):\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    for curr_row in test_data[:]:\n",
    "        word, splitted_list = split_to_list(curr_row)\n",
    "        removed_stress, target = check(splitted_list)\n",
    "        data_list.append(word + ':' + ' '.join(removed_stress))\n",
    "        target_list.append(target)\n",
    "    \n",
    "    return data_list, target_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuckfuckrounds(raw_data, epoches):\n",
    "    for i in range(epoches):\n",
    "        print(\"Now round \" + str(i))\n",
    "        print(\"Start time : \" + time.asctime())\n",
    "        train_data, test_data = train_test_split(raw_data, test_size = 0.1)\n",
    "        data_list, target_list = get_test_data(test_data)\n",
    "        submission.train(train_data, classifier_path)\n",
    "        pred_res = submission.test(data_list, classifier_path)\n",
    "        acc = accuracy_score(target_list, pred_res)\n",
    "        f1 = f1_score(target_list, pred_res, average='macro')\n",
    "        precision = precision_score(target_list, pred_res, average='macro')\n",
    "        recall = recall_score(target_list, pred_res, average='macro')\n",
    "        print(\"Testing accuracy : \" + str(acc))\n",
    "        print(\"Precision : \" + str(precision))\n",
    "        print(\"Recall : \" + str(recall))\n",
    "        print(\"F1 : \" + str(f1))\n",
    "        print(\"End time : \" + time.asctime())\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now round 0\n",
      "Start time : Sat May 20 12:11:23 2017\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-111187a35eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./asset/classifier.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfuckfuckrounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvowel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f6a838038db4>\u001b[0m in \u001b[0;36mfuckfuckrounds\u001b[0;34m(raw_data, epoches)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpred_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/COMP9318/proj_spec/finalfinal/submissionLRvowel2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, classifier_file)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mvowel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure_dict_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvowel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsonant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvowel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/COMP9318/proj_spec/finalfinal/submissionLRvowel2.py\u001b[0m in \u001b[0;36mget_features_train\u001b[0;34m(raw_data, vowel, consonant)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurr_vowel_position\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvowel_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mprefix_suffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prefix_and_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_vowel_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_stress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsonant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mrow_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure_dict_2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venturer/COMP9318/proj_spec/finalfinal/submissionLRvowel2.py\u001b[0m in \u001b[0;36mget_prefix_and_suffix\u001b[0;34m(vowel_positions, removed_stress, consonant)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prefix_and_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvowel_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_stress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsonant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcurr_position\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvowel_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurr_position\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# no prefixes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprefix1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "classifier_path = './asset/classifier.dat'\n",
    "fuckfuckrounds(vowel2, epoches = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
