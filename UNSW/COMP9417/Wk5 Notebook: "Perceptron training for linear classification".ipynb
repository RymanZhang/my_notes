{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Perceptron training for linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this lab we will expand on some of the concepts of linear\n",
    "classification using the perceptron. We start with a look into the representational capacity of a perceptron, then how to implement learning, and finally look at a perceptron learning a linear classifier on a real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This lab is based on the presentation and code in Chapter 3 of ``Machine Learning'' by Stephen Marsland, CRC Press, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this lab we will use a slight variant on the notation and setup\n",
    "used in the lectures.\n",
    "These changes are not going to affect the capabilities of the perceptron.\n",
    "\n",
    "For a given set of $m$ inputs, the first stage of the computation is when the perceptron  multiplies each of the input values with its corresponding weight and adds these together:\n",
    "\n",
    "$$ h = \\sum_{i}^{m} w_{i} x_{i} $$\n",
    "\n",
    "The second stage is to apply the thresholding output rule or activation function of the perceptron to produce the classification output.\n",
    "\n",
    "For this lab we will slightly change the activation function to map to either $0$ or $1$ rather than the $-1$ or $+1$ we had in the lecture notes.\n",
    "\n",
    "The value set for the bias or threshold input will also be changed from $1$ to $-1$.\n",
    "\n",
    "$$ o = g(h) = \\left\\{\n",
    "                \\begin{array}{lll}\n",
    "                        1 & \\mbox{if}           & h > 0 \\\\\n",
    "                        0 & \\mbox{otherwise if} & h \\leq 0 \\\\\n",
    "                \\end{array}\n",
    "              \\right. $$\n",
    "\n",
    "Let's go ahead and implement a Perceptron in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Representing simple Boolean functions as a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will first look at modelling a simple two-input Boolean function as linear classifier. This is a Perceptron WITHOUT any learning! To get started we will use the OR function, for which the truth table will be familiar to you all. Note that you will need to pick some weights for the function to output the correct values given the input. There are many possible values that could do the job. Also, remember to take care with the dimension of the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Input [0, 0] with Class 0 Predict  0\n",
      "For Input [0, 1] with Class 1 Predict  1\n",
      "For Input [1, 0] with Class 1 Predict  1\n",
      "For Input [1, 1] with Class 1 Predict  1\n"
     ]
    }
   ],
   "source": [
    "# set up the data, i.e., all the cases in the truth table \n",
    "x=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y=[0,1,1,1]\n",
    "# number of data points\n",
    "n=4\n",
    "# number of inputs to the perceptron\n",
    "m=3\n",
    "# what weights should be assigned to correctly compute the OR function ?\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "# e.g., w=[-0.09,0.14,0.01]\n",
    "# w = [-0.09,0.14,0.01] is a wrong weight for that x[0,0] outputs 1\n",
    "# how can you come up with a suitable set of weights ?\n",
    "w = [1 , 1.1, 1.1]\n",
    "# loop over the data\n",
    "for i in range(n):\n",
    "    h=w[0]*(-1)# this is the bias weight and input\n",
    "    for j in range(1,m):\n",
    "        h+=w[j]*x[i][j-1]\n",
    "    if(h>0):\n",
    "        output=1\n",
    "    else:\n",
    "        output=0\n",
    "    print 'For Input', x[i], 'with Class', y[i], 'Predict ', output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now change your code to model the AND function (again restricted to two inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Input [0, 0] with Class 0 Predict  0\n",
      "For Input [0, 1] with Class 0 Predict  0\n",
      "For Input [1, 0] with Class 0 Predict  0\n",
      "For Input [1, 1] with Class 1 Predict  1\n"
     ]
    }
   ],
   "source": [
    "# set up the data, i.e., all the cases in the truth table \n",
    "x=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y=[0,0,0,1]\n",
    "# number of data points\n",
    "n=4\n",
    "# number of inputs to the perceptron\n",
    "m=3\n",
    "# what weights should be assigned to correctly compute the AND function ?\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "# how can you come up with a suitable set of weights ?\n",
    "w = [1,1,1]\n",
    "# loop over the data\n",
    "for i in range(n):\n",
    "    h=w[0]*(-1)# this is the bias weight and input\n",
    "    for j in range(1,m):\n",
    "        h+=w[j]*x[i][j-1]\n",
    "    if(h>0):\n",
    "        output=1\n",
    "    else:\n",
    "        output=0\n",
    "    print 'For Input', x[i], 'with Class', y[i], 'Predict ', output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Changing the data structures for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We got right down to the details of how a linear classifier works. Now this being a perceptron, you probably recall that rather than using a fixed set of weights to do the prediction each time, there is a simple training rule that updates the weights on the basis of discrepancies between the classifier's prediction on the data and the actual class. So we could extend our previous code to implement that training rule, but the code is a little fiddly and you're probably thinking there should be a simpler way to do this. If so, you are correct, but it is based on moving towards coding with matrix and vector operations, rather than directly using Python arrays. To do this we need to import the NumPy library (there is a tutorial at: <href <a>https://docs.scipy.org/doc/numpy-dev/user/quickstart.html</a>>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For example, when we need to predict a class for an instance $\\mathbf{x}$ given the current weights $\\mathbf{w}$ we can use the inner product operation $\\mathbf{x} \\cdot \\mathbf{w}$. To get this functionality using NumPy we just do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([0,1,1])\n",
    "w=np.array([0.02,0.03,0.03])\n",
    "\n",
    "h=np.dot(x,w)\n",
    "print h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "But wait, there's more! Since $\\mathbf{x}$ and $\\mathbf{w}$ are both actually matrices, the same operation will enable us to apply the inner product of the weight vector $\\mathbf{w}$ to ALL the data instances at once. In this case we write the matrix of data instances $\\mathbf{X}$. Just note that we need to take care that the data matrix and weight vector are properly initialised to make this operation work correctly. Now the code for predicting the class values of all of our data given the weight vector is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "nData = \n",
      "4\n",
      "Now X is \n",
      "[[ 0.  0. -1.]\n",
      " [ 0.  1. -1.]\n",
      " [ 1.  0. -1.]\n",
      " [ 1.  1. -1.]]\n",
      "Activations:\n",
      "[[-0.02]\n",
      " [ 0.01]\n",
      " [ 0.01]\n",
      " [ 0.04]]\n",
      "Predictions:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Misclassifications\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data set with class values in last column\n",
    "dataset = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]]) # OR function\n",
    "X=dataset[:,0:2]\n",
    "print \"X = \"\n",
    "print X\n",
    "y = dataset[:,2:]\n",
    "print y\n",
    "# Note: the bias weight is now the last!\n",
    "w = np.array([[0.03],[0.03],[0.02]])\n",
    "# Add the values for the bias weights (-1) to the data matrix\n",
    "nData = np.shape(X)[0]\n",
    "print \"nData = \"\n",
    "print nData\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "print \"Now X is \"\n",
    "print X\n",
    "# get the value of the activation function\n",
    "h = np.dot(X,w)\n",
    "yhat = np.where(h>0,1,0)\n",
    "err = yhat-y\n",
    "\n",
    "print 'Activations:\\n', h\n",
    "print 'Predictions:\\n', yhat\n",
    "print 'Misclassifications\\n', err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code uses some more NumPy built-ins. Check the documentation to be sure you know what is going on. One of these, np.where(), is useful here. It takes 3 arguments and returns an array. The first argument is a predicate on an array that is either evaluates to true, returning the second argument at the corresponding index in the array or false, returning the third argument instead. Now see how you get on re-implementing the code to do the prediction for the two-input Boolean AND function, as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations:\n",
      "[[-1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Predictions:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "Misclassifications\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data set with class values in last column\n",
    "# dataset for AND function\n",
    "dataset = np.array([[0,0,0],[0,1,0],[1,0,0],[1,1,1]])\n",
    "X=dataset[:,0:2]\n",
    "y = dataset[:,2:]\n",
    "# Note: the bias weight is now the last!\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "w = np.array([1,1,1]).reshape((3,1))\n",
    "# Add the values for the bias weights (-1) to the data matrix\n",
    "nData = np.shape(X)[0]\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "# get the value of the activation function\n",
    "h = np.dot(X,w)\n",
    "yhat = np.where(h>0,1,0)\n",
    "err = yhat-y\n",
    "\n",
    "print 'Activations:\\n', h\n",
    "print 'Predictions:\\n', yhat\n",
    "print 'Misclassifications\\n', err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Adding in weight updates to make the learning work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have spent some time just getting the weights and data in the right vector-matrix format to be able to do the prediction. What else do we need to get this thing to learn ?\n",
    "\n",
    "One thing we will need is some random initialisation for the weight vector. What sort of values would be appropriate for this initialisation?\n",
    "\n",
    "The initialisation will be done using a NumPy built-in. Note that we need weights for each of the inputs \"nIn\", plus one for the bias. Also, the \"nOut\" parameter is just a placeholder in case you want your Perceptron to predict more that one output at a time. Here we will just use one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00594308]\n",
      " [ 0.03581867]]\n"
     ]
    }
   ],
   "source": [
    "# w = np.random.rand(nIn+1,nOut)*0.1-0.05 # Check: does this return a column vector?\n",
    "w = (np.random.rand(1+1,1) * 0.1-0.05).reshape((2,1)) \n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The other main thing we need is to see how the Perceptron training rule is implemented to update the weights for each attribute given all the information in the data matrix plus the misclassifications. Note that this implementation is a batch version, unlike the version in the lecture notes which is incremental. Both approaches have their place. Here we go for simplicity of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What must the inner dimensions of the matrix multiplcation be for the weight update ? Check with the lecture notes to see what terms we will need. Recall that the augmented data matrix has $m+1$ columns, where $m$ is the number of inputs. However, the misclassifications, or errors, are of dimensionality $n$, because there is potentially one misclassification for every example in the dataset. What has to happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Correct: you need to transpose the augmented data matrix to ensure the inner dimensions match (they both must be of size $n$). Check you are sure before inspecting the code (it's just a one-liner). Here the parameter \"eta\" is the learning rate $\\eta$, which for this code is set to $0.25$. Once more $\\hat{y} - y$ will be our misclassification vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a054c7c6434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# this is it - learning in one line of code!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eta' is not defined"
     ]
    }
   ],
   "source": [
    "w -= eta*np.dot(np.transpose(X),yhat-y)# this is it - learning in one line of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can put it all together. Note that we need to set an upper limit for the number of iterations (T). Play with this code and run it as above for our Boolean functions. See what happens to the weights for \"OR\". Does the Perceptron learn this function? Now try \"AND\". Then try \"XOR\" (exclusive or). Now go back and experiment with the learning rate. Does anything change ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0  Error: 0.5\n",
      "Iteration: 1  Error: 0.5\n",
      "Iteration: 2  Error: 0.5\n",
      "Iteration: 3  Error: 0.5\n",
      "Iteration: 4  Error: 0.5\n",
      "Iteration: 5  Error: 0.5\n",
      "Iteration: 6  Error: 0.5\n",
      "Iteration: 7  Error: 0.5\n",
      "Iteration: 8  Error: 0.5\n",
      "Iteration: 9  Error: 0.5\n",
      "Iteration: 10  Error: 0.5\n",
      "Iteration: 11  Error: 0.5\n",
      "Iteration: 12  Error: 0.5\n",
      "Iteration: 13  Error: 0.5\n",
      "Iteration: 14  Error: 0.5\n",
      "Iteration: 15  Error: 0.5\n",
      "Iteration: 16  Error: 0.5\n",
      "Iteration: 17  Error: 0.5\n",
      "Iteration: 18  Error: 0.5\n",
      "Iteration: 19  Error: 0.5\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "# Dataset with class values in last column\n",
    "#dataset = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]])   # OR function\n",
    "#dataset = np.array([[0,0,0],[0,1,0],[1,0,0],[1,1,1]]) # dataset for AND function\n",
    "dataset = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,0]]) # dataset for XOR function --> need at least 2 layers!\n",
    "X = dataset[:,0:2]\n",
    "y = dataset[:,2:]\n",
    "nIn = np.shape(X)[1]    # no. of columns of data matrix\n",
    "nOut = np.shape(y)[1]   # no. of columns of class values -- just 1 here\n",
    "nData = np.shape(X)[0]  # no. of rows of data matrix\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "eta=0.25\n",
    "T=20\n",
    "# Train for T iterations\n",
    "for t in range(T):\n",
    "        # Predict outputs given current weights\n",
    "        h = np.dot(X,w)\n",
    "        yhat = np.where(h>0,1,0)\n",
    "        # Update weights for all incorrect classifications\n",
    "        w -= eta*np.dot(np.transpose(X),yhat-y)\n",
    "        # Output current performance\n",
    "        errors=yhat-y\n",
    "        perrors=((nData - np.sum(np.where(errors==0,1,0)))/nData)\n",
    "        #print perrors, 'is Error on iteration:', t\n",
    "        print 'Iteration:', t, ' Error:', perrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Perceptron training on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, try this out on a real dataset, the Pima diabetes dataset. You can download this from within your program. The rest of your program should work the same. Replace the lines defining the dataset, X and y variables with the code below. You might want to increase the number of iterations from 20 as well. Unfortunately, this basic implementation of neural learning is not likely to find a very good model. Try transforming the data, for example, by make all attribute values lie in the same range. Search for methods of normalisation using the NumPy built-in functions \"np.mean()\" and \"np.var()\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "# URL for the Pima Indians Diabetes dataset (UCI Machine Learning Repository)\n",
    "url = \"http://goo.gl/j0Rvxq\"\n",
    "# download the file\n",
    "raw_data = urllib.urlopen(url)\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape) # 8 attributes, 1 class, 768 examples\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8:9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is the full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "Iteration: 0  Error: 0.651041666667\n",
      "Iteration: 1  Error: 0.348958333333\n",
      "Iteration: 2  Error: 0.348958333333\n",
      "Iteration: 3  Error: 0.651041666667\n",
      "Iteration: 4  Error: 0.348958333333\n",
      "Iteration: 5  Error: 0.490885416667\n",
      "Iteration: 6  Error: 0.348958333333\n",
      "Iteration: 7  Error: 0.651041666667\n",
      "Iteration: 8  Error: 0.348958333333\n",
      "Iteration: 9  Error: 0.557291666667\n",
      "Iteration: 10  Error: 0.348958333333\n",
      "Iteration: 11  Error: 0.38671875\n",
      "Iteration: 12  Error: 0.352864583333\n",
      "Iteration: 13  Error: 0.651041666667\n",
      "Iteration: 14  Error: 0.348958333333\n",
      "Iteration: 15  Error: 0.649739583333\n",
      "Iteration: 16  Error: 0.348958333333\n",
      "Iteration: 17  Error: 0.352864583333\n",
      "Iteration: 18  Error: 0.649739583333\n",
      "Iteration: 19  Error: 0.348958333333\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "import urllib\n",
    "# URL for the Pima Indians Diabetes dataset (UCI Machine Learning Repository)\n",
    "url = \"http://goo.gl/j0Rvxq\"\n",
    "# download the file\n",
    "raw_data = urllib.urlopen(url)\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape) \n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8:9]\n",
    "\n",
    "nIn = np.shape(X)[1]    # no. of columns of data matrix\n",
    "nOut = np.shape(y)[1]   # no. of columns of class values -- just 1 here\n",
    "nData = np.shape(X)[0]  # no. of rows of data matrix\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "eta=0.25\n",
    "T=20\n",
    "# Train for T iterations\n",
    "for t in range(T):\n",
    "        # Predict outputs given current weights\n",
    "        h = np.dot(X,w)\n",
    "        yhat = np.where(h>0,1,0)\n",
    "        # Update weights for all incorrect classifications\n",
    "        w -= eta*np.dot(np.transpose(X),yhat-y)\n",
    "        # Output current performance\n",
    "        errors=yhat-y\n",
    "        perrors=((nData - np.sum(np.where(errors==0,1,0)))/nData)\n",
    "        #print perrors, 'is Error on iteration:', t\n",
    "        print 'Iteration:', t, ' Error:', perrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As before, this code is just Python code so you can use this for further experimentation. Of course, it can be improved a lot!  A good starting point would be the NumPy documentation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
